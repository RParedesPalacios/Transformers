{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU 0\n",
    "workers_id = \"0\"\n",
    "\n",
    "# Use GPU 1\n",
    "#workers_id = \"1\"\n",
    "\n",
    "# Use GPU 0 and 1 (Multi-GPU)\n",
    "#workers_id = \"0,1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def set_gpus_info(workers_id):\n",
    "\n",
    "    DEVICE = None\n",
    "    MULTI_GPU = None # flag to use multiple GPUs\n",
    "    GPU_ID = None # specify your GPU ids\n",
    "\n",
    "    if workers_id == 'cpu' or not torch.cuda.is_available():\n",
    "        GPU_ID = []\n",
    "        print(\"check\", workers_id, torch.cuda.is_available())\n",
    "    else:\n",
    "        GPU_ID = [int(i) for i in workers_id.split(',')]\n",
    "    if len(GPU_ID) == 0:\n",
    "        DEVICE= torch.device('cpu')\n",
    "        MULTI_GPU = False\n",
    "    else:\n",
    "        DEVICE= torch.device('cuda:%d' % GPU_ID[0])\n",
    "        if len(GPU_ID) == 1:\n",
    "            MULTI_GPU = False\n",
    "        else:\n",
    "            MULTI_GPU = True\n",
    "    \n",
    "    print(\"Number of GPUs: \", len(GPU_ID))\n",
    "    print(\"Device \", DEVICE)\n",
    "\n",
    "    return DEVICE, MULTI_GPU, GPU_ID \n",
    "\n",
    "\n",
    "DEVICE, MULTI_GPU, GPU_ID  = set_gpus_info(workers_id)\n",
    "\n",
    "num_workers = len(GPU_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "# You can use your own pre-trained HuggingFace models in this function\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "            \"google/vit-base-patch16-384\", \"roberta-base\", tie_encoder_decoder=True \n",
    "        )\n",
    "\n",
    "from transformers import ViTFeatureExtractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Si queremos usar un tokenizador (BPE) entrenado por nosotros:\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('MY_BPE_FOLDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.max_length = 20\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Image_Captioning_Dataset(Dataset):\n",
    "    def __init__(self, partition=\"train\", tokenizer=None, feature_extractor=None):\n",
    "    \n",
    "        self.l_captions = []\n",
    "        self.l_images = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        self.image_path = None\n",
    "        self.caption_path = None\n",
    "        if partition == \"train\":\n",
    "            self.image_path = \"./data_ved/Images_train/\"\n",
    "            self.caption_path = \"./data_ved/train_captions.txt\"\n",
    "        else:\n",
    "            self.image_path = \"./data_ved/Images_test/\"\n",
    "            self.caption_path = \"./data_ved/test_captions.txt\"\n",
    "\n",
    "        # Using readlines()\n",
    "        file1 = open(self.caption_path, 'r')\n",
    "        Lines = file1.readlines()\n",
    "        count = 0\n",
    "\n",
    "        # Strips the newline character\n",
    "        for line in Lines:\n",
    "            count += 1\n",
    "            line = line.strip()\n",
    "            captions_images = line.split(',')\n",
    "            self.l_images.append(self.image_path + captions_images[0])\n",
    "            self.l_captions.append(captions_images[1])\n",
    "\n",
    "        self._max_label_len = max([8] + [len(label) for label in self.l_captions])\n",
    "        print(\"Loaded \", count, \" samples!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Shape (h,w,3) and 1...255\n",
    "        image = Image.open(self.l_images[idx]).convert(\"RGB\")\n",
    "        image_tensor: torch.Tensor = self.feature_extractor(image, return_tensors=\"pt\").pixel_values[0]\n",
    "\n",
    "        label = self.l_captions[idx]\n",
    "        label_tensor = self.tokenizer(\n",
    "            label,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=self._max_label_len,\n",
    "        ).input_ids[0]\n",
    "\n",
    "        return {\"idx\": idx, \"input\": image_tensor, \"label\": label_tensor}\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = Image_Captioning_Dataset(partition=\"test\", tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "train_dataset = Image_Captioning_Dataset(partition=\"train\", tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "batch_size = 1\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, num_workers=num_workers)\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "print(\"ViT input: \", test_dataset.__getitem__(0)[\"input\"].shape)\n",
    "print(\"RoBERTa input: \", test_dataset.__getitem__(0)[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0)\n",
    "\n",
    "from transformers import get_scheduler\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "if MULTI_GPU:\n",
    "    print(\"Multi gpu \", MULTI_GPU)\n",
    "    # multi-GPU setting\n",
    "    model = nn.DataParallel(model, device_ids = GPU_ID)\n",
    "    model = model.to(DEVICE)\n",
    "else:\n",
    "    print(\"single gpu\")\n",
    "    # single-GPU setting\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_loss = 0 \n",
    "    print(\"\\nEpoch \", epoch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with tqdm(iter(train_dataloader), desc=\"Training set\", unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "\n",
    "            inputs: torch.Tensor = batch[\"input\"].to(DEVICE)\n",
    "            labels: torch.Tensor = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(pixel_values=inputs, labels=labels)\n",
    "\n",
    "            loss = outputs.loss.to(DEVICE)\n",
    "            loss = loss.sum().to(DEVICE) # for MULTI-GPU\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Optimizer\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.data.item())\n",
    "\n",
    "            epoch_loss += loss.data.item()\n",
    "                \n",
    "            del loss, outputs\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        epoch_loss = epoch_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"Loss: \", epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(\n",
    "    device, multi_gpu, tokenizer, feature_extractor, model: VisionEncoderDecoderModel, dataloader: DataLoader\n",
    ") -> tuple[list[tuple[int, str]], list[float]]:\n",
    "\n",
    "    l_generated_text = []\n",
    "    if multi_gpu:\n",
    "        model = model.module # unpackage model from DataParallel\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Predicting\")):\n",
    "            \n",
    "            inputs: torch.Tensor = batch[\"input\"].to(device)\n",
    "\n",
    "            generated_ids = model.generate(inputs)\n",
    "            generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            l_generated_text.append(generated_text)\n",
    "\n",
    "    return l_generated_text\n",
    "\n",
    "model.eval()\n",
    "l_generated_text = predict(DEVICE, MULTI_GPU, tokenizer, feature_extractor, model, test_dataloader)\n",
    "l_generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('trocr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "889e2ae9d4f379e97a6d6a83d8646eed2366d764f411c139907d1e4ee22fe0b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
